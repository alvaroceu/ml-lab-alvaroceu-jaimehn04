{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 3: Jaime Héctor y Álvaro Sánchez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, es necesario realizar un preprocesamiento de los datos para garantizar que estén limpios, completos y en un formato adecuado que permita al modelo de machine learning aprender de manera eficiente. Para ello se deben evitar los valores nulos, normalizar las variables numéricas y codificar las variables categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Cargar los datos en una variable\n",
    "datos_test = pd.read_csv(\"public_test.csv\")  # Fichero CSV con datos de test\n",
    "datos_train = pd.read_csv(\"public_train.csv\")  # Fichero CSV con datos de entrenamiento\n",
    "\n",
    "# Separar la información del resultado ovariable objetivo\n",
    "x_train = datos_train.drop(columns=\"ccs\")  # 'ccs' es la columna objetivo\n",
    "y_train = datos_train[\"ccs\"]\n",
    "x_test = datos_test\n",
    "\n",
    "# Iniciar transmormadores para adaptar los datos al modelo\n",
    "scaler = StandardScaler()\n",
    "numeric_imputer = SimpleImputer(strategy=\"mean\")  # Nulo numérico --> Media del resto\n",
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")  # Nulo categórico --> Moda del resto\n",
    "oh_encoder = OneHotEncoder(sparse_output=False)  # One-hot encoding: Codificar variables categóricas\n",
    "\n",
    "# Transformador numérico (Eliminar nulos + normalizar)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", numeric_imputer),\n",
    "    (\"scaler\", scaler)\n",
    "])\n",
    "\n",
    "# Transformador categórico (Eliminar nulos + codificar)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", categorical_imputer),\n",
    "    (\"encoder\", oh_encoder)\n",
    "])\n",
    "\n",
    "# Agrupar las diferentes columnas relevantes de los datos\n",
    "desc_columns = [col for col in x_train.columns if col.startswith('desc_')]\n",
    "fgp_columns = [col for col in x_train.columns if col.startswith('fgp_')]\n",
    "adduct_column = [\"adduct\"]  # 'adduct' es categórica \n",
    "\n",
    "# Transformar usando ColumnTransformer, para elegir a que columnas aplicar cada tipo de transformación\n",
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numeric\", numeric_transformer, desc_columns + fgp_columns),  # Numéricos\n",
    "        (\"categorical\", categorical_transformer, adduct_column)  # Categóricos\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Aplicar los cambios realizados en los datos (no los resultados)\n",
    "x_train = transformer.fit_transform(x_train)\n",
    "x_test = transformer.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y estimación del error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para Regresión lineal se obtiene una mediana de error: 3.7602733080887845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "# Dividir los datos: 70% entrenamiento, 20% validación\n",
    "x_train_final, x_val, y_train_final, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "model1 = LinearRegression()\n",
    "\n",
    "model1.fit(x_train_final, y_train_final)\n",
    "\n",
    "y_val_pred1 = model1.predict(x_val)\n",
    "mae_val = median_absolute_error(y_val, y_val_pred1)\n",
    "    \n",
    "print(f\"Para Regresión lineal se obtiene una mediana de error: {mae_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para Random Forest con 5 estimadores se obtiene una mediana de error: 3.483719999999991\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model2 = RandomForestRegressor(5)\n",
    "\n",
    "model2.fit(x_train_final, y_train_final)\n",
    "\n",
    "y_val_pred2 = model2.predict(x_val)\n",
    "mae_val = median_absolute_error(y_val, y_val_pred2)\n",
    "    \n",
    "print(f\"Para Random Forest con 5 estimadores se obtiene una mediana de error: {mae_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para Random Forest con 10 estimadores se obtiene una mediana de error: 3.25837300000002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model3 = RandomForestRegressor(10)\n",
    "\n",
    "model3.fit(x_train_final, y_train_final)\n",
    "\n",
    "y_val_pred3 = model3.predict(x_val)\n",
    "mae_val = median_absolute_error(y_val, y_val_pred3)\n",
    "    \n",
    "print(f\"Para Random Forest con 10 estimadores se obtiene una mediana de error: {mae_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m fold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_val_indices, test_indices \u001b[38;5;129;01min\u001b[39;00m outer_kfold\u001b[38;5;241m.\u001b[39msplit(x_train, y_train):\n\u001b[1;32m---> 40\u001b[0m     x_train_val \u001b[38;5;241m=\u001b[39m x_train\u001b[38;5;241m.\u001b[39miloc[train_val_indices]\n\u001b[0;32m     41\u001b[0m     y_train_val \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39miloc[train_val_indices]\n\u001b[0;32m     42\u001b[0m     x_test_fold \u001b[38;5;241m=\u001b[39m x_train\u001b[38;5;241m.\u001b[39miloc[test_indices]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Dividir los datos: 70% entrenamiento, 15% validación, 15% test\n",
    "x_train_final, x_temp, y_train_final, y_temp = train_test_split(x_train, y_train, test_size=0.3, random_state=42)\n",
    "x_val, x_test_train, y_val, y_test_train = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Bucle interno para selección de hiperparámetros\n",
    "def fit_model(x_train_final, y_train_final, model):\n",
    "    inner_kfold = KFold(n_splits=5, shuffle = True,random_state=42)\n",
    "    maes = []\n",
    "    for train_indices, val_indices in inner_kfold.split(x_train_final):\n",
    "        x_train = x_train_final.iloc[train_indices]\n",
    "        y_train = y_train_final.iloc[train_indices]\n",
    "        x_val = x_train_final.iloc[val_indices]\n",
    "        y_val = y_train_final.iloc[val_indices]\n",
    "\n",
    "        model.fit(x_train, y_train)\n",
    "        y_val_pred = model.predict(x_val)\n",
    "        maes.append(median_absolute_error(y_val, y_val_pred))\n",
    "\n",
    "    avg_mae = np.mean(maes)\n",
    "\n",
    "    return avg_mae\n",
    "\n",
    "# Modelos a comparar\n",
    "models = [\n",
    "    (\"Linear Regression\", LinearRegression()),\n",
    "    (\"Random Forest\", RandomForestRegressor(5))\n",
    "]\n",
    "\n",
    "# Bucle externo para validación cruzada\n",
    "outer_kfold = KFold(n_splits=5, shuffle = True ,random_state=42)\n",
    "results = []\n",
    "fold = 1\n",
    "for train_val_indices, test_indices in outer_kfold.split(x_train, y_train):\n",
    "    x_train_val = x_train.iloc[train_val_indices]\n",
    "    y_train_val = y_train.iloc[train_val_indices]\n",
    "    x_test_fold = x_train.iloc[test_indices]\n",
    "    y_test_fold = y_train.iloc[test_indices]\n",
    "\n",
    "    print(f\"Fold {fold}\")\n",
    "    for name, model in models:\n",
    "        print(f\"Evaluando modelo: {name}\")\n",
    "        avg_mae = fit_model(x_train_val, y_train_val, model)\n",
    "\n",
    "        model.fit(x_train_val, y_train_val)\n",
    "        y_test_pred = model.predict(x_test_fold)\n",
    "        mae_test = median_absolute_error(y_test_fold, y_test_pred)\n",
    "        print(f\"MAE en el conjunto  (Fold {fold}): {mae_test}\")\n",
    "\n",
    "        results.append((name, fold, avg_mae, mae_test))\n",
    "\n",
    "    fold += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg_mae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(avg_mae)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_mae' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos visto que usando Random Forest de 10 estimadores obtenemos los mejores resultados segun la MAE, volvemos a entrenar este modelo pero con todos los datos de entrenamiento (public_train.csv). Si usasemos más estimadores el porcentaje de error mejoraría pero tardaría demasiado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model3.predict(x_test)\n",
    "\n",
    "df_resultados = pd.DataFrame(y_pred)\n",
    "\n",
    "df_resultados.to_csv('test_preds.csv', index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
